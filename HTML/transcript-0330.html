<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <title>Transcription: Podcast 0330 - Stephan Schmitt</title>

  <!-- Bootstrap -->
  <link href="css/bootstrap.min.css" rel="stylesheet">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js does not work if you view the page via file:// -->
  <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->

</head>

<body>
  <div class="container">
    <h2>Transcription: 0330 - Stephan Schmitt</h2>
    <h3>Released: June 14, 2020</h3>
    <br><p>
      <iframe style="border: none" src="//html5-player.libsyn.com/embed/episode/id/14815331/height/90/theme/custom/thumbnail/yes/direction/forward/render-playlist/no/custom-color/000000/" height="90" width="100%" scrolling="no"  allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen></iframe>
    </p><br>
    <p>
      <b>Darwin: </b>
      <i>Okay, today I get a chance to talk to somebody who I'm actually really excited to speak to. His name is Stephan Schmitt, and he is the brains behind Nonlinear Labs, which creates this mind boggling synth called the C15. But he has a long history that has been in my interest area. He's one of the founders of Native Instruments. He has a long history of development work for people who work in the Reaktor world. And I have a lot of history in that world as well. So I'm very excited to get a chance to talk to Stephan. Hey Stephan, how are you?
      </i>
    </p>

    <p>
      <b>Stephan Schmitt: </b>
      I'm fine. Thanks for having me and considering me.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah. Well, thank you so much for taking time out to do this - I appreciate it. We're getting a chance to talk on a Saturday morning for me - a Saturday evening for you - but, I appreciate you taking some of your personal time to have it, have this discussion. For people who might not be aware of what Nonlinear Labs is, why don't we start off by having you explain a little bit about your company? </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah. Nonlinear Labs is a smaller company by intention. So we are a five to 10 people. We are developing only one product and it's a niche product, in a way, because it's addressing musicians that are really into keyboard playing and expressive playing of keyboard instruments and some unique feature. So there are some unique concepts that we have behind the whole thing. And
      of course it's based on experience from my past. So I bring experience with software instruments, and it is an instrument that we are building now. It is just a dedicated self-contained instrument. That is not meant to be the universal - let's say workstation - but [rather] something with a real character. So I'm very old school instrument building. This is our instrument manufacturing, that we try to do here. We also try to work with the local companies or European companies.
    </p>

    <p>
      Mainly we try to be also in direct contact with our customers. So we do direct sales and, you know, this is ending up OK. It's not a very low cost product. It's more on the pricier side also because we are a German company producing locally, assembling the thing at our place here and dealing with the customers directly. So taking really good care of the users, getting them training and so on. It's kind of the opposite of what I have done before. So it's something, it's a passion thing for me. The product is very much defined by my own preferences and ideas. And that's why, yeah, it is also a niche thing. So, from my personal preferences, I'm not sharing say with the mass markets, it's more like some musicians who has a similar idea of what is the quality of an instrument,
    </p>

    <p>
      <b>Darwin: </b>
      <i>Right? I remember the first time that you showed this at a NAMM show, I was talking to a friend of mine and I was like, "What did you see at at the show that amazed you the most?" And this is a guy - who's pretty jaded, so he doesn't get amazed at very much. And he said, "I saw the Nonlinear Labs C15. And I can't tell if it's brilliant or crazy!" Because, first of all, he said, "When I touch it, it seems amazing that the things that you've done to make it feel expressive..." - it blew him away. But he's said it's oriented toward keyboard players instead of MIDI producers. And he's like, "I don't even know if there are enough keyboard players left to be able to enjoy something like this!" </i>
    </p>

    <p>
      <b>Stephan: </b>
      Exactly. This is all a risk, let's say. We are addressing buyers who are making... I would say the number of buyers getting smaller and smaller potential. But on the other hand, I feel it like for me, it's a statement - it's also kind of a mission. And I try to also to convince young people that playing live, playing in real time and playing it expressively is really worth it to learn and to invest time, right? Because the most valuable thing is time that you have to invest. And I would like, for example, I would like to, convince jazz players and there are many young piano players. And so to try to go the electronic way, of sounds- not an electronic way of producing, but just the sound creation. Now it's a sound engine being a digital electronic thing.
    </p>

    <p>
      This fascination has been in the seventies, eighties, with people like Herbie Hancock and Chick Corea and so on, they explored all the possibilities of electronic equipment and instruments. Or Joe Zawinul and so on. But today, the just seen as very conservative, but I'm on a mission. To convince this sort of people, and that also when I, when we came to a NAMM show, we really enjoyed to meet a lot of good players, mainly from the black music scene. And they also loved the instruments. So it's this field of also funk music and Neo-Soul that instruments really fit into, and I think this will not die.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah, well, I think that you're onto something. And the fact of the matter is I think that people have got this vision that all music comes from sample CDs and goes through a DAW in order to be made. But when you actually talk to people - especially in the hip hop scene right now - there is this real resurgence to have a group in the studio jamming and doing a great funk track to build a hip hop track off. Or you have bands like The Roots and some of these others that are  really pushing the idea of what live musicianship actually brings to the enjoyment of music. And I think it's really interesting. I also love this idea that you're on a mission to sort of re-establish electronic sounds into jazz. I agree with you: it's like, it used to be that the most amazing explorations in electronic sound were Zawinul, Hancock and Korea, where these people were taking the instrument far more than commercial designers or rock music was. It was the jazz guys that were really pushing the envelope. And it seems like trying to find a way to encourage that world to take on electronic sounds again is a pretty neat, goal. </i>
    </p>

    <p>
      <b>Stephan: </b>
      I mean, it's kind of an experiment, it surprises a lot of people that I bring an instrument to the market that is electronic, and it's not designed primarily for the electronic music or this other style, but it works like - let's say like a natural or like a classical instrument. And  then it comes to this discussion that we, you know, we don't have MIDI implemented. Which is trying to focus on, on this domain of playing - of having a very expressive and sensitive keyboard and, you know, to make the best out of that. So that the engine is responsive. It's about the responsiveness to the realtime thing. And, yeah, so we are in the domain of instrument building like nearly all the others - except the synthesizer!
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah. </i>
    </p>

    <p>
      <b>Stephan: </b>
      The guitar makers, the drum makers and even the piano makers also, I mean, it's about the outside of the instrument. We have it covered completely in wood. So it's also an organic touch that you get from the thing. And maybe it looks a little bit too much like a furniture thing, because it can be really a... let's say an angry beast!
    </p>

    <p>
      <b>Darwin: </b>
      <i>You're right: with the wood on it, it looks like it would be a very mellow thing. And then when I looked at the synth engine, I mean, you're talking about a system that has wave-shaping in the core of its being. I'm like, "I have a feeling that this is going to sound pretty ripping."
      </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah. We showed it to a composer in Hollywood and also said, "Oh, this sounds different than it looks."
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah, no doubt. So one of the things that I like doing on my podcast is talking to people about their background and how they got to be the creators that they are. And in your case, you have a very interesting background because you ccome at this hardware synth development from very much a software background. And so I'm kind of curious about what that path looked like for you, how you got started, where your interest in synths came from in the first place, how you got started working on digital synthesis, how you worked your way into and through the Native Instruments world - and how you came out of it needing to do a hardware instrument. I'm really curious about this path. Could you tell me a little bit about that? </i>
    </p>

    <p>
      <b>Stephan: </b>
      Okay. So I'm now the age of 62. So my young years, when I was a boy, it was in the sixties and seventies. In those days, I was very much into technology or technical tinkering. And I got some components for electronic circuits and books around that. And so everything that I saw, and tried out was, for me, it was fascinating when it produced sound. So this was the main goal when I was 12 years old, 10 years old, and I was doing something, I was interested how I can make this thing sound. And so I learned about electronics as much as I could get the information. And later I started studying electrical engineering. So in '76, I started studying in Brunswick at a big technical university, and this in the first place doesn't have to do with audio and acoustics and stuff like that. It was a really serious engineering from the energy or radio transmission stuff or whatever - everything that's electrical, let's say. At the same time, during studies, I was also a member of some bands.
    </p>

    <p>
      So I was fascinated by rock music and jazz-rocks also. And also some world music - working with African people. So in these bands, I was the keyboard player. I didn't have a good education as a keyboardist. So I had some lessons on the piano when I was young, but this is... Until today, this is the thing that I miss. I tried to get to be a better player. Actually, I currently am taking piano lessons again and trying to really improve, but I was coming, of course, from the technical side. And I had some knowledge I could apply in the bands. I was also doing sound reinforcement stuff, mixing live, mixing in studios for bands. So I was always on the border between technical and musical activities.
    </p>

    <p>
      It was taking up a lot of time and my studies were suffering. And it took quite long until I finished study. So in 88, I finished and I started a job in Berlin. So this is how I came into the city. I started in the communications industry developing or being a member of the development team for fiber optics transmissions. I was always more like the analog guy. So fiber optics is a very high speeds, but it's analog again - so you're dealing with photo diodes and
      stuff like that. That is where the signal and the noise - yeah, you have to get the maximum out of signal, and the minimum of noise. So that was my first job. And then I was there for two or three years. And by this time I was still very much fascinated by music and by also by digital instruments.
    </p>

    <p>
      Actually, all kinds of instruments, but it was the time off of the DX7, and, as somebody with a
      electrical engineering background, the DX7 was not so scary for me, like for others. And, I had quite a number of synthesizers and keyboards when I was a student. I sold them all for one... I bought a CP-70, a Yamaha piano. This was like, I thought - okay, to be serious about music means playing piano. Piano is enough. Don't be so distracted by all this technical stuff. So it stays with the piano. [And] I learned that I will never become a relevant piano player! And what I was more able was, of course, electronics and signal theory stuff. And that's why I decided for the DX7 around the end of my studies.
    </p>

    <p>
      And then when I was living in Berlin, I was playing the DX7 only. I was trying to find my own style, my own way to play it. I made some solo concerts with improvisations on a DX7 and some effect devices - and also very small audience like in galleries and the artist environments, not so much the commercial music one. And, by this, I found some sort of contact with dancers, and they hired me to make the music and play with them when they dance. It was smaller dance stuff. And so I got quite some stage experience by this, but in a very experimental - let's say non-mainstream - sort of music. But yeah, I quit my job to spend more time on music.
    </p>

    <p>
      After a while, of course, my money was used up and I got another job that was really a nice fit for me. They offered me to join a team that was developing mixing consoles. And the special thing was, it was people from the East, from East Germany, because [it was] 1990 when the wall was down and Berlin was reunited. And so, the Western companies were looking for what are the qualified people in the East and interesting projects in the East? And so one company was interested in this team that had developed all the audio equipment for the East German broadcasting system. And so they had a computer-controlled mixing system - not finished, but in development. And so I was hired to join them. And so this was a very exciting time, to develop a monster mixing console with people from the East, and also to learn about the German history and way of thinking. And by the way, brilliant engineers, because they had to work with much weaker components than we had available there. To really make things go up which are nearly impossible. They make the
      impossible possible.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah. Kind of magic, magical engineering, right.</i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah. And so in this time, because it was a computer-controlled mixing system, I had to deal with a lot of components. And so my colleagues in the software domain, they got input from me because I was leading the project and I had to specify it and we had to test it. And so for me, I got more and more in touch with software. For me, it was clear that also audio was already going in this direction would be software more and more. So, I thought, "Okay, I have to train myself to become a software developer.", because I was mainly an analog hardware engineer.
    </p>

    <p>
      And I met one guy, actually, he was just this student, intern or something. He was also into music. And when we talked about it, he was interested in joining a project that I was imagining. This was okay: let's learn how to make sound sources in software, because this is the most exciting thing - sound processing, like with effects or with mixers, you know, it's not the source, it's just a later stage. And if you can finish the product or you can make it somewhat [better] well... But the most exciting is where the sound comes from. So where the oscillations start from, where the waveform is created. The software project was also after two, two and a
      half years or so, it was canceled because money was cut down and predict political situations...
    </p>

    <p>
      So I don't know. Actually, today, I would say it was a monster project that didn't have a big chance to survive a long time. Of course, lots of experience in development, for all of us. This project ended, and I convinced the East German colleagues to start a little project on our own at home to find out if we can do something that synthesizes just as software project. And at that moment, the power of the desktop computers or notebook computers was not sufficient to get, say, more than one channel of effect or a stereo effect working. So we first started on a DSP system, so that we have the full freedom of the software design. And, so we can do this on DSPs.
    </p>

    <p>
      Maybe later we can do this also on main processors, on native processes. And actually, this was quite soon the case. So Windows 95 was available. So it was a 32-bit operating system, the Pentium 5 was out, which was also a powerful professor. I think Dave Smith had some influence on some small aspect for signal processing there because Intel was also trying to position this processor, because it was so powerful, for realtime calculations in the field of audio, in the field of modems of those days. And so how to move some of the same peripherals, like sound cards and modems, into the processor - this was the idea. And that was the name for native signal processing that they introduced: "NSP". It didn't survive very long because I think the industry was not willing to follow them, but I had this software library, or I had to look at the software library and the name gave me also the idea for the Native Instruments from the foundation later.
    </p>

    <p>
      So we forgot about the DSP, let's go for a really native signal processing and the way that we use ordinary desktop computers and the law of Moore telling them that power is doubling. It was true for a long time. So these hardware companies, they worked for us and the way that they made our creations more and more powerful, possible with the guarantee that after two years... Those days there were no startups. So we needed some time to learn and to find out how we can create this. And when we were at the market, the processes and the systems fast enough to create a convincing instrument.
    </p>

    <p>
      This was, I think, '96, the showed the first version of Generator, which was the predecessor of Reaktor, at MusiK Messe, the big German trade show. And we got some press, which was taking it in the way that I was also excited about. I felt - okay, this is the next big thing. So I was very much promoting the advantages of software synthesis and the flexibility that you get, user interfaces, graphical user interfaces, complete freedom there, the full integration into sequencers and so on. And the biggest argument, of course, is price.
    </p>

    <p>
      You get instruments, or you can create instruments, and the quality that was not affordable in hardware, for maybe a few hundred or 200 euros for a plugin. And this is, of course, this is a big factor that you can save so much, or you can create a really serious sound sources for smaller money. So, this trade show in '96 gave us some publicity. So the press was giving us some space telling us this is the most exciting or most interesting thing, upcoming thing. And by this, some people noticed, and they asked to join us. So we were able to build up a team with people who are even more qualified in software development domain that we were. And so this was the start for Native Instruments. So '96, this was the first official year of Native Instruments. In the beginning there were two people with this, a colleague and me, and then one year later, we
      were six, seven people who became also the founding shareholders, later, of this company.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Well, this is really an amazing story because, I mean, you were actually one of the people responsible for what I think is one of the most important pushes in musical software development. About the time that you started was also about the time that the VST plugin environment was put out there. And the problem at the time to me was that the quality of the software-based instruments did not compare with the hardware equivalents, right? It was fascinating to see things that would work solely inside of your computer, but the sound was not anywhere near expectations. And I was in a lucky position to work at a magazine that was... I was writing for a magazine and we got a copy of Generator in for a review. And I remember the first time I fired it up and made a very simple software synth. </i>
    </p>

    <p>
      <i>I remarked to my editor at the time that this was one of the first things I had heard on the computer that actually sounded like an instrument that I would want to use, you know, and subsequently, all of that kind of work on making good sounding stuff, not only led to Generator becoming Reaktor, becoming this great software synth development system, but all of that technology was also used for things like, the original B4 and the P5 and some of these earlier emulations, since they were head and shoulders, sonically, improvements over any of the software instruments that had been previously available. </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah. In the beginning, I thought that would be our only product because I had this idea of having a universal construction kit. For also empowering people that they can do their own
      instruments. This is, of course, expecting too much from the users. And so we had to learn that offering something that, you know, from as a developer who's developing software, it's good to be modular in a way you have to have modularity and to recombine your components and to make them reusable. And so I wanted to give this, or to providers also on the user interface, because I thought, okay, people can do this themselves. They'll then make it like their own instrument. And this was an illusion in some ways. So, the most successful products of Native Instruments were let's say the ready-built instruments like the B4 and the prebuilt instruments.
    </p>

    <p>
      And, and of course they have also other possibilities to optimize the DSP, and also the user interface, have a dedicated concept like an organ or synth or whatever. But in the beginning it was Reaktor. What was the big use of actual them or how useful it was? It was as a development tool app - [we] benefited from having a system where you can develop very fast something that sounds okay. And if you invest a bit more time, it sounds really good. We had the Reaktor core, we had access to every, let's say operation, so you can, the question of sound is only a question of knowhow. See what you can do low level programming as you would write a program in C. And so it was attracting some very good instrument designers. So we had people joining in, in the first, in the community of Reaktor users that we also had established a forum and a place for exchanging the creations. We met some very talented people there who became part of Native Instruments, later developing instruments. And doing this in Reaktor as the tool for  prototyping and developing.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah. You know, I actually, I was involved with Native Instruments in some of the early days. I helped write the manual for the B4, and I helped with the P5 manual and stuff. And I remember that for helping with writing with the manual, the test devices I would get were actually customized versions of Reaktor, that had the organ voicings and stuff like that. It was very much that the system was used as your internal development system as well. </i>
    </p>

    <p>
      <b>Stephan: </b>
      It could be also a development system let's say for third parties, but Native never established, I'd say kind of a marketplace also for that, but we had the idea to have it also for third party developers as a platform. They could create their instruments and also make their own business on that.
    </p>

    <p>
      <b>Darwin: </b><i>Yeah. Although I think that the way that you did the patch exchange system for Reaktor was it was one of the real successes. With the Reaktor platform was having this huge shared library that people could create things for. And, you know, I know a number of people over the years that were Reaktor users that primarily spent their time surfing through the library of shared patches and looking for new stuff. That was kind of the constant area of exploration for them. They didn't really program, they mostly just kind of surfed other people's programming, but a very powerful system. Now, in addition to the potential for people doing sharing and some of this other stuff, you also were involved in creating some Reaktor-based synths that were sold as instruments, you know, things like Spark and Prism and Scanner - I think a lot of people will have, will have good memories of working with those systems as well. What compelled you to do to do those instruments?  </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah, I tried also to get back to making music myself. So I had the problem over many years. But I was so much involved with the music technology that I was not able to make any music anymore, because I would always think about the tools instead of the music and about bug fixing or improvements or, in the mind of a developer. But I was able, after a while, when the company was running well, to spend more time with music and I was creating my own instrument, I had a MIDI master keyboard and the notebook in the rehearsal room. And I was trying to make mainly music, but on the other hand, of course, creating instruments that make sense. And in my same  application and  Spark was one thing that was developed in this way.
    </p>

    <p>
      <b>Darwin: </b><i>Literally just from you playing around in your rehearsal room? </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah. I mean, playing, playing, and playing andtesting the thing. So when you play with guitar players, for example, you'll notice - okay, how well is this thing standing up against that? And also spending spending many hours on just say modifying. So I started the concept that's, I don't know, for the Contour, which later became, the C15 sound engine. I started with a four operator FM structure, and I combined this with some filtering and some distortion and shaping stuff. And then I tried to keep this all, so I don't want to, to be the universal thing, but with some of the parameters, it should fit on one screen, should be usable and easy to access for me. And therefore I try to keep it minimalistic in a way.
    </p>

    <p>
      Also the four operators became two operators, but while two of the FM operators became shapers. And, and then I learned more about the comb filter, or how to work with comb filters, with Prism. We worked on model synthesis, but, combining FM and comb filters, for example, is was quite effective and worked out well. And that's the reason why I was using, say, the predecessor of contour, which was called Phase22. Phase modulation, two oscillators, two filters.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Oh, okay. Got it. </i>
    </p>

    <p>
      <b>Stephan: </b>
      It was a thing of evolution, you know? I had this for years already in, I don't know, hundreds of different versions. And then a colleague, he was the lead of sound design at Native. He asked me, can you, maybe you can share it with us and we can make a product out of it. And this happened several times. And, so we produced some of these products - they are innovative, they are similar. They are... Skanner, I mean, it's based on samples, but reading samples with a sine wave, something really special. And it has a characteristic sound. It's not something that works like a universal tool. Prism has this, let's say, this very physical and metallic sounding character
    </p>

    <p>
      <b>Darwin: </b>
      <i>Again, not a universal tool, right? </i>
    </p>

    <p>
      <b>Stephan: </b>
      And Contour, in the end, the last thing that I did for Native when I really had started Nonlinear Labs, it's a smaller version of what you have now in the C15. As the same concepts, like get how to get metallic sound, how to get percussive sounds, how to get - it's a lot of influence of the velocity, which is one of the main modulation or expresses a parameter and how to make it work for real time control with pedals and ribbons and whatever you touch. I'm grateful that I could build this instrument, based on the actual and being like an evolutionary path to what we can offer to date.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Sure. So, let's talk a little bit about the design of the C15, because again, this is one that people who've experienced it have been pretty, pretty shocked by it. If you go read reviews that people have written about them, inevitably what they talk about is the surprising level of variety and sound that can come out of something that, on the surface might seem like a simple system design, but it's one that kind of has a lot of built-in complexity. You have this kind of FM-based system with a shaper built in, but also, heavily focused on kind of there being a feedback circuit in the whole instrument design. It seems like when I look at the engine circuit diagram, one of the things that I see is the whole top of the diagram is all all the stuff that's part of the feedback system. </i>
    </p>

    <p>
      <i>And so feedback is clearly, I think, one of the things that anybody who's into media arts and especially the technology of media, knows is that feedback is where a lot of magic lies, right? There's always something magical that occurs when you allow feedback to be part of the system. So in the design of this system, it's interesting that you talked about in your background, you had this transition where you went from the the CP-70 to the DX7, right? And then that became kind of the place where you found your electronic voice, you were doing these solo concerts and being kind of experimental. And then when we talk about some of the things that you did in later years with Native kind of really exploring these metallic in these, and in these more aggressive tones, it strikes me that you have kind of a special place in your heart for the combination of FM and interesting feedback-based circuits. How did that inform the C15? I mean, did you come into it experimenting or did you kind of know this is the engine that I want to, I want to explore?
      </i>
    </p>

    <p>
      <b>Stephan: </b>
      I had the luck to try out of course, a lot of designs and to work with a lot of very talented instrument designers. So, of course, the market already has provided a lot of choices. Let's say, yeah, you can try out what Yamaha did for FM, Korg did something in the domain also of physical modeling with the Prophecy and that one and, or the Oasis system, or the Wavedrum, by the way, one of the really very interesting products. And so I was always, I'm looking for systems that have their own logic and which have a small number of components and small, let's say,  information built in; I don't like when instruments come with samples and when they come with sequences built in, I don't want anything to be predefined and prerecorded or pre-produced.
    </p>

    <p>
      I want that the sound is generated in the milliseconds when you hit the key, and it's fascinating to see how complex FM can get with only a few sine waves involved, because of the complex partials that it breaks up into. And it's also fascinating to see how complex something can get, if you have a delay, a feedback part, and nonlinearity. It's something that was also explored by chaos theory. So in showing how a system can get chaotic, if you think it's a quiet phase, very predictable, simple system that you are dealing with, only a few variables in the few, simple formulas. And you end up with a chaotic behavior that cannot be predicted anymore. So that's fascinating for me to have a structure in our place it's to find ways oscillators and filters and yeah, some routing now that you can crossfade and mix, to make up output mixing passes, but also feedback mixing partners. Right. And that we can create, in the C15, we can create on the exciter side, we can create something like a noise, a short burst. And on the resonator side, we have a delayed base converter that when it's excited, it already can create a rich sound if you want, and you can play it because it's precisely tuneable. And this is not invented by us, but it's, something that's very effective for.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Well, I think that that's fascinating because I think that comb folders - people really think of comb filters as being a very small case thing. You'll use it occasionally for doing resonant chords or whatever, but I think people don't think of it as having the potential for being a general use tool. And I love the fact that you have kind of brought this comb filter in is kind of part of the heart of the sound engine of the C15. </i>
    </p>

    <p>
      <b>Stephan: </b>
      I think you can run the C15 in different, let's say, configurations. If you can use only those two oscillators, so you can run it subtractive engine. But for me also, the heart is the concept of because you get these resonances. If you combine this with allpass, you get also non-harmonic content there. And if, when it comes to self-exciting, by feedback loops, you get into a behavior that is real can be really surprising. Not only the classical, let's say from what you know from guitar feedback. But energy is coming back into the input of the filter and how this is also changing sometimes slowly. It's really, yeah - as surprising as you think, okay, this is only a short delay and the feedback, why should that have such an evolving behavior? So it is also very interesting that you only have to detune one of the components a little bit and you get dramatic change.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Right? Yeah. Well, that's probably something you learned a little bit too in working with the DX7. I know the first time I got a DX7 and I went down the rabbit hole of programming it, I too had a technical background. And so, I wasn't that intimidated by it, but what I did notice is the way that very small parameter changes could have very huge effects on the output, because again of the effect of both what FM does with subtle changes and what feedback does with subtle changes - really small parameter changes can have extraordinary effects. </i>
    </p>

    <p>
      <b>Stephan: </b>
      I mean, that's part of also expressiveness, I mean, if you think about, say assigning velocity to certain components of the classical analog synth. You can change the sound, but it's not changed so dramatically as it would do with the FM synthesizer where the spectrum can completely break. Yeah. If you can get from a very mellow to a very aggressive sound and by just driving some, or having some parameters driven a little bit higher, and that's also what happens in this combination of a feedback and FM - that's something. I was, you know, I was a keyboard player in bands with guitar players and they had the big noise or the big drama, the big feedback thing on there. And I want to give the keyboard player something back!
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah. Right. It's giving them something to compete with that. That's great. So, the one thing that nobody's going to let me get away without asking is to talk about your decision not to include MIDI. I think it's a brave decision. I think it's one that firmly puts you in the position of making a niche instrument. To what extent did you do that so that you did not have to compromise the engine? </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah, that's, that's one aspect is that MIDI is okay. Today there's MIDI 2.0 and we can talk about higher resolutions. But, MIDI is - let's say it's an old protocol. It works very well in many cases, but when it comes for example to resolution, and also the velocity, it's not all satisfying, but the main thing for me is that when you start to think about an instrument as a self-contained thing, it should sound rich and satisfying and inspiring by itself. When you include MIDI, you start to think about the controller and expander, master keyboard functionality. You're building up a system that is more like your studio being your instrument. This is what I hear often.
    </p>

    <p>
      <b>Darwin: </b>
      <i>And then you're back looking at general case instruments again, right? </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah. You know, we spent a lot of energy and time on building up the system, and this complex configuration. And I wanted something that is really not distracting or is really focused.  Focused on playing, not to think too much about how this can be automated and so on. It's really, it's about the focus of the user. And it was also about the focus of us as the maker of the instrument, of course, to not to think about, this could be also nice expander and we could offer master keyboard functionaly. The challenge is to put the thing that people really love to play, and they don't miss anything externally and they get lost. They hopefully get lost in the playing instead of getting lost in studio wiring.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Right. Well, that's a beautiful concept and I really can appreciate that. And again, when I look forward to hearing more players get the C15 under their fingers because it strikes me that this is a real, it's an opportunity to open up a new kind of playing, instead of just always thinking of synthesizers being kind of a studio environment thing. Again, you can start also thinking about a synthesizer as being a live instrument. And you said something earlier that just really resonated to me, which is that the way that you're building the C15 is kind of like every other instrument in the world - other than the synthesizer! You're building it - I think you're almost building modern harpsichords or something, right. It's something for a particular flavor and a particular kind of player who's going to end up needing the thing that you're making. So I think that that's really cool. And I applaud you for doing that. </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah, thanks. It reminds me of some of the Clavinet.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Oh yeah. </i>
    </p>

    <p>
      <b>Stephan: </b>
      It's also from the size and also from some parts of the sound character, being very... it's dynamic, aggressive. And yeah, and that's why I think we can address the black music scene, but actually not only as I mentioned jazz musicians and even this domain, but we have quite a number of customers in the domain of soul music. So composers, typically they are trained to play the piano. So they are not missing the MIDI input. And they love the colors that they can get, the the special amount of fresh sorts of sound that they can get out of this instrument. Also, we have electronic producers who often tend to record audio and work with audio loops and snippets. And so they get inspired by the C15 by playing something on it. They record as they sing, and later they cut out something and they use this for their production.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Yeah. So there's ways to fit it into the modern studio, too. That's really a great point. So unfortunately our time is up and in fact, we went quite a ways over, but I so enjoy talking to you about this. And, I have a million more questions that I wish who we could get to, but before I let you go, I know that you just came out with an update to the sound engine and some of the programmability of the C15. It was going to be shown at Superbooth, but you ended up being in  the virtual Superbooth, where you showed off a little bit of those capabilities. Can you talk about what that update represented, and then what, what you imagine to be the next things that are likely to see an upgrade. </i>
    </p>

    <p>
      <b>Stephan: </b>
      We were able to optimize our sound engine and operating system in the way that you get more voices and also get more flexibility out of it. And this we use in the first update. Now we use, providing a dual mode, which means we can split up, the 24 voices into two times 12, which can be layered or split on the keyboard. So with the layering, of course, you get a richer sounds and also an easy way to do sound design by combining components. There was mono mode missing up to now. So we'll be able to providing a much more normal solo playing in the latest update. And we have quite a number of parameters edits, tremolo is available now. And, yeah, the feedback system, especially in the layer mode, gets really some more options to route route signals from one voice to the other, and back to the first one.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Oh, cross feedback. Oh my goodness. </i>
    </p>

    <p>
      <b>Stephan: </b>Feedback to the voices, yeah. And so, we expect this, yeah, and they already experienced it being a powerful addition, now. There will be a further evolution of this engine, but we also think about a completely different sound engines. This product is meant to be a sustainable. It means the hardware would be the basis also for other DSP running on it. We just have to replace the magnetic foils in future. And then it could support a completely other set of parameters.
    </p>

    <p>
      <b>Darwin: </b>
      <i>Oh, wow. That's interesting. </i>
    </p>

    <p>
      <b>Stephan: </b>
      Yeah. That's why it looks a little bit, it's a confusing for people to see so many buttons and only one encoder. But these buttons are the way how to also access the parameters and they are laid out in a generic way, so it can support something, maybe a completely other thing, like an additive sound engine. So there are some ideas, already in preparation. But as we are small team, it will take some time before we come over with the next, different, engine.
    </p>

    <p>
      <b>Darwin: </b><i>Well, it sounds like it, it sounds like a beautiful way to work though. And, I love the fact that you've built this thing from the ground up as kind of a kind of a platform for, for experiencing the work that you're doing. But again, also the focus that you're bringing to it. I really love that. And, I appreciate so much you sharing your vision with, with me and with the listeners. Thank you so much.
      </i>
    </p>

    <p>
      <b>Stephan: </b>
      Thank you. It was a very nice to talk to you.
    </p>

    <p>
      <b>Darwin: </b><i>Yeah. Likewise. Okay. With that, then I will let you have the rest of your evening. </i>
    </p>

    <p>
      <b>Stephan: </b>Yeah. Thank you. Goodbye.
    </p>

    <p><i>Copyright 2020 by Darwin Grosse. All right reserved.</i></p>
  </div>
  <!-- jQuery (necessary for the Bootstrap JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="js/bootstrap.min.js"></script>

</body>

</html>
